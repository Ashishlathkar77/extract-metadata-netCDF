{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tW4RLVT1zZsH",
        "outputId": "318accb2-b4bb-4ff4-b688-1aaec5a5c00e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (2.9.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.3 in /usr/local/lib/python3.10/dist-packages (from pydantic) (2.23.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic) (4.12.2)\n",
            "Collecting netCDF4\n",
            "  Downloading netCDF4-1.7.1.post2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting cftime (from netCDF4)\n",
            "  Downloading cftime-1.6.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from netCDF4) (2024.8.30)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from netCDF4) (1.26.4)\n",
            "Downloading netCDF4-1.7.1.post2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cftime-1.6.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: cftime, netCDF4\n",
            "Successfully installed cftime-1.6.4 netCDF4-1.7.1.post2\n",
            "Requirement already satisfied: xarray in /usr/local/lib/python3.10/dist-packages (2024.6.0)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.10/dist-packages (from xarray) (1.26.4)\n",
            "Requirement already satisfied: packaging>=23.1 in /usr/local/lib/python3.10/dist-packages (from xarray) (24.1)\n",
            "Requirement already satisfied: pandas>=2.0 in /usr/local/lib/python3.10/dist-packages (from xarray) (2.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0->xarray) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0->xarray) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0->xarray) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0->xarray) (1.16.0)\n",
            "Libraries imported\n"
          ]
        }
      ],
      "source": [
        "!pip install pydantic\n",
        "!pip install netCDF4\n",
        "!pip install xarray\n",
        "\n",
        "from netCDF4 import Dataset\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List, Optional\n",
        "import os\n",
        "import xarray as xr\n",
        "\n",
        "# Display when done\n",
        "print('Libraries imported')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NetCDFMetadata(BaseModel):\n",
        "    dimensions: dict = Field(..., description=\"Dimensions of the NetCDF file.\")\n",
        "    variables: List[str] = Field(..., description=\"Variables available in the NetCDF file.\")\n",
        "    attributes: dict = Field(..., description=\"Global attributes of the NetCDF file.\")\n",
        "    file_name: str = Field(..., description=\"The name of the NetCDF file.\")\n",
        "\n",
        "# Display when done\n",
        "print('NetCDFMetadata model created')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hFAhoJ7zesk",
        "outputId": "645b8eba-25a7-4603-d2d8-66886095f934"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NetCDFMetadata model created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_netcdf_metadata(file_path: str) -> NetCDFMetadata:\n",
        "    with Dataset(file_path, 'r') as nc:\n",
        "        dimensions = {dim: len(nc.dimensions[dim]) for dim in nc.dimensions}\n",
        "        variables = list(nc.variables.keys())\n",
        "        attributes = {attr: nc.getncattr(attr) for attr in nc.ncattrs()}\n",
        "        file_name = os.path.basename(file_path)\n",
        "        return NetCDFMetadata(\n",
        "            dimensions=dimensions,\n",
        "            variables=variables,\n",
        "            attributes=attributes,\n",
        "            file_name=file_name\n",
        "        )\n",
        "\n",
        "# Display when done\n",
        "print('Metadata extraction function created')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeA8G3wTzmjG",
        "outputId": "2681bd51-779b-42ab-92d7-203beb79eb76"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metadata extraction function created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List your NetCDF files\n",
        "netcdf_files = ['/content/gom_t008.nc']\n",
        "\n",
        "# Extract metadata for each file\n",
        "all_metadata = [extract_netcdf_metadata(f) for f in netcdf_files]\n",
        "\n",
        "# Display the extracted metadata\n",
        "for metadata in all_metadata:\n",
        "    print(metadata)\n",
        "\n",
        "# Display when done\n",
        "print('Metadata extraction completed')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7yTJ06wzoDV",
        "outputId": "bbd7aeae-ccec-4446-ea08-741265a29a35"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dimensions={'lat': 346, 'lon': 541, 'depth': 40, 'time': 1} variables=['time', 'tau', 'depth', 'lat', 'lon', 'water_u', 'water_v', 'water_temp', 'salinity', 'surf_el'] attributes={'classification_level': 'UNCLASSIFIED', 'distribution_statement': 'Approved for public release; distribution unlimited.', 'downgrade_date': 'not applicable', 'classification_authority': 'not applicable', 'institution': 'Naval Oceanographic Office', 'source': 'HYCOM archive file', 'history': 'archv2ncdf3z', 'field_type': 'instantaneous', 'Conventions': 'CF-1.6 NAVO_netcdf_v1.1'} file_name='gom_t008.nc'\n",
            "Metadata extraction completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install llama-index\n",
        "#!pip install chromadb\n",
        "#!pip install openai\n",
        "#!pip install llama-index-vector-stores-chroma\n",
        "\n",
        "from llama_index.core import StorageContext, VectorStoreIndex, Settings\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "import chromadb\n",
        "import os\n",
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = 'sk-RlyK9UahfWCbcQJcz7iEHqkxvCNj9vVHREOhSrRuS0T3BlbkFJIMns3sTSMPRlgx4f1x-k-XGkmmXO3EQFSejYlcD2YA'\n",
        "\n",
        "# Set up LlamaIndex Settings\n",
        "Settings.llm = OpenAI(model='gpt-4o-mini', temperature=0.1)\n",
        "Settings.embed_model = OpenAIEmbedding()\n",
        "\n",
        "# Chroma settings\n",
        "chroma_path = './chroma_db'\n",
        "chroma_collection_name = 'chrm'\n",
        "\n",
        "# Display when done\n",
        "print('LlamaIndex components loaded')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7qJ_8g4zszi",
        "outputId": "dd75da13-f705-437c-de52-d939d33a9091"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LlamaIndex components loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load or create vector store\n",
        "if os.path.exists(chroma_path):\n",
        "    chroma_client = chromadb.PersistentClient(path=chroma_path)\n",
        "    chroma_collection = chroma_client.get_or_create_collection(chroma_collection_name)\n",
        "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "    index = VectorStoreIndex.from_vector_store(vector_store)\n",
        "    print('Vector store loaded')\n",
        "else:\n",
        "    chroma_client = chromadb.PersistentClient(path=chroma_path)\n",
        "    chroma_collection = chroma_client.get_or_create_collection(chroma_collection_name)\n",
        "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "\n",
        "    # Ensure documents are properly formatted\n",
        "    documents = [{'text': doc.text, 'metadata': doc.metadata} for doc in all_metadata]\n",
        "\n",
        "    # Create the index\n",
        "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "    index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)\n",
        "    print('Vector store created')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qOCtZVh0VEg",
        "outputId": "c6e576f5-5864-4450-f79a-13a15a4d5054"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector store loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.retrievers import VectorIndexAutoRetriever\n",
        "from llama_index.core.vector_stores.types import MetadataInfo, VectorStoreInfo\n",
        "\n",
        "# Prepare metadata schema\n",
        "all_metadata_info = []\n",
        "for field_name, field_info in NetCDFMetadata.__fields__.items():\n",
        "    all_metadata_info.append(\n",
        "        MetadataInfo(\n",
        "            name=field_name,\n",
        "            type=str(field_info.annotation),\n",
        "            description=field_info.description,\n",
        "        )\n",
        "    )\n",
        "\n",
        "vector_store_info = VectorStoreInfo(\n",
        "    content_info=\"list of NetCDF files metadata\",\n",
        "    metadata_info=all_metadata_info,\n",
        ")\n",
        "\n",
        "retriever = VectorIndexAutoRetriever(index, vector_store_info, verbose=True)\n",
        "print('Metadata schema prepared')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qN361Z3i1zmg",
        "outputId": "a1ec3921-09f9-47dc-feb0-948c95047be9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metadata schema prepared\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "\n",
        "def retry_request(func, retries=3, delay=5):\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            return func()\n",
        "        except Exception as e:\n",
        "            print(f\"Attempt {attempt + 1} failed: {e}\")\n",
        "            time.sleep(delay)\n",
        "    raise RuntimeError(\"All retry attempts failed.\")\n",
        "\n",
        "# Set up the query engine\n",
        "query_engine = RetrieverQueryEngine.from_args(retriever=retriever, streaming=True)\n",
        "\n",
        "def make_query():\n",
        "    return query_engine.query('What variables are in the first NetCDF file?')\n",
        "\n",
        "# Retry the query request\n",
        "try:\n",
        "    resp = retry_request(make_query)\n",
        "    for token in resp.response_gen:\n",
        "        print(token, end=\"\")\n",
        "except Exception as e:\n",
        "    print(f\"Query failed: {e}\")\n",
        "\n",
        "print('Query executed')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuphdE3A18-U",
        "outputId": "efd25540-c984-469d-9e01-07e7e83fc8f5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using query str: variables in the first NetCDF file\n",
            "Using filters: []\n",
            "Empty ResponseQuery executed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import netCDF4\n",
        "\n",
        "# Check variables in the first NetCDF file\n",
        "file_path = '/content/gom_t008.nc'\n",
        "dataset = netCDF4.Dataset(file_path, 'r')\n",
        "\n",
        "print(\"Variables in the NetCDF file:\")\n",
        "for var in dataset.variables:\n",
        "    print(var)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85hTcC4f1-h_",
        "outputId": "ba2a5981-9763-4efd-d5b9-626223a97d42"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variables in the NetCDF file:\n",
            "time\n",
            "tau\n",
            "depth\n",
            "lat\n",
            "lon\n",
            "water_u\n",
            "water_v\n",
            "water_temp\n",
            "salinity\n",
            "surf_el\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f7k55fbg3bOK"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XOpv2vVb37Dj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import netCDF4 as nc\n",
        "\n",
        "def extract_metadata(netcdf_file):\n",
        "    dataset = nc.Dataset(netcdf_file)\n",
        "    metadata = {\n",
        "        'variables': list(dataset.variables.keys()),\n",
        "        'dimensions': list(dataset.dimensions.keys()),\n",
        "        'attributes': {attr: getattr(dataset, attr) for attr in dataset.ncattrs()}\n",
        "    }\n",
        "    dataset.close()\n",
        "    return metadata\n",
        "\n",
        "# Example of extracting metadata\n",
        "metadata = extract_metadata('/content/gom_t008.nc')\n",
        "print(metadata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARk7pRNU37Hr",
        "outputId": "a38f8146-fd67-47c1-cfa7-fdb93961ca18"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'variables': ['time', 'tau', 'depth', 'lat', 'lon', 'water_u', 'water_v', 'water_temp', 'salinity', 'surf_el'], 'dimensions': ['lat', 'lon', 'depth', 'time'], 'attributes': {'classification_level': 'UNCLASSIFIED', 'distribution_statement': 'Approved for public release; distribution unlimited.', 'downgrade_date': 'not applicable', 'classification_authority': 'not applicable', 'institution': 'Naval Oceanographic Office', 'source': 'HYCOM archive file', 'history': 'archv2ncdf3z', 'field_type': 'instantaneous', 'Conventions': 'CF-1.6 NAVO_netcdf_v1.1'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Store metadata as JSON\n",
        "metadata_json = json.dumps(metadata, indent=4)\n",
        "with open('metadata.json', 'w') as f:\n",
        "    f.write(metadata_json)"
      ],
      "metadata": {
        "id": "6_scv2Dx4DPa"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain openai langchain_community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5I4hR9l4F4q",
        "outputId": "1137d7cd-afec-49a0-abf3-0d0956b0d0c3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.2.16)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.44.1)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.2.16-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.34)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.38 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.39)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.4)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.118)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.5.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.5.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.11.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.8)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.22.0)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.38->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.38->langchain) (24.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.23.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.38->langchain) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
            "Downloading langchain_community-0.2.16-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain_community\n",
            "Successfully installed langchain_community-0.2.16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "# Initialize OpenAI chat-based LLM (using gpt-3.5-turbo)\n",
        "chat_llm = ChatOpenAI(temperature=0.1, model=\"gpt-3.5-turbo\")\n",
        "\n",
        "# Create a prompt template to query the metadata\n",
        "template = \"\"\"\n",
        "You are an assistant that helps users explore metadata of NetCDF files. Below is the metadata:\n",
        "\n",
        "{metadata}\n",
        "\n",
        "Answer the following question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"metadata\", \"question\"],\n",
        "    template=template\n",
        ")\n",
        "\n",
        "# Create a chain to query the metadata\n",
        "metadata_chain = LLMChain(llm=chat_llm, prompt=prompt)\n",
        "\n",
        "# Example of querying metadata\n",
        "query = \"What are the variables in the NetCDF file?\"\n",
        "response = metadata_chain.run({\n",
        "    \"metadata\": metadata_json,\n",
        "    \"question\": query\n",
        "})\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EPH_zbH4IyK",
        "outputId": "5adb63ec-58d2-445e-d85b-a3f41e526446"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-27-c4c0c846fa24>:6: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
            "  chat_llm = ChatOpenAI(temperature=0.1, model=\"gpt-3.5-turbo\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The variables in the NetCDF file are: \n",
            "1. time\n",
            "2. tau\n",
            "3. depth\n",
            "4. lat\n",
            "5. lon\n",
            "6. water_u\n",
            "7. water_v\n",
            "8. water_temp\n",
            "9. salinity\n",
            "10. surf_el\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List of queries you want to test\n",
        "queries = [\n",
        "    \"What are the variables in the NetCDF file?\",\n",
        "    \"What is the time range of the data in the NetCDF file?\",\n",
        "    \"What is the spatial resolution of the data in the file?\",\n",
        "    \"Are there any missing values in the NetCDF file?\",\n",
        "    \"What is the depth range covered in this dataset?\"\n",
        "]\n",
        "\n",
        "# Loop through each question and query the metadata\n",
        "for query in queries:\n",
        "    response = metadata_chain.run({\n",
        "        \"metadata\": metadata_json,\n",
        "        \"question\": query\n",
        "    })\n",
        "\n",
        "    print(f\"Question: {query}\")\n",
        "    print(f\"Response: {response}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvnRKAWw4Mtk",
        "outputId": "399d331b-fb1d-4ce8-eeae-ccf8568ffe28"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What are the variables in the NetCDF file?\n",
            "Response: The variables in the NetCDF file are:\n",
            "1. time\n",
            "2. tau\n",
            "3. depth\n",
            "4. lat\n",
            "5. lon\n",
            "6. water_u\n",
            "7. water_v\n",
            "8. water_temp\n",
            "9. salinity\n",
            "10. surf_el\n",
            "\n",
            "Question: What is the time range of the data in the NetCDF file?\n",
            "Response: The time variable in the NetCDF file represents the time dimension. Without specific values provided, we cannot determine the exact time range of the data in the file.\n",
            "\n",
            "Question: What is the spatial resolution of the data in the file?\n",
            "Response: The spatial resolution of the data in the file can be determined by looking at the dimensions \"lat\" and \"lon\". In this case, the dimensions are lat and lon, which typically represent latitude and longitude coordinates. The resolution of the data would depend on the spacing between the latitude and longitude values in the dataset. Unfortunately, the specific values for the latitude and longitude spacing are not provided in the metadata, so the exact spatial resolution cannot be determined without further information.\n",
            "\n",
            "Question: Are there any missing values in the NetCDF file?\n",
            "Response: Based on the provided metadata, we cannot determine if there are any missing values in the NetCDF file. The metadata only includes information about the variables, dimensions, and attributes of the file, but does not specify whether there are missing values in the data itself. To determine if there are missing values, you would need to examine the actual data within the NetCDF file.\n",
            "\n",
            "Question: What is the depth range covered in this dataset?\n",
            "Response: The depth range covered in this dataset is determined by the \"depth\" variable, which is one of the dimensions in the NetCDF file. The depth dimension in this dataset ranges from shallow to deep depths. The specific depth values within this range would need to be examined in the actual data to determine the exact depth range covered.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xHjGdWmn4rjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import netCDF4 as nc\n",
        "\n",
        "# Function to extract metadata from the NetCDF file\n",
        "def extract_metadata(nc_file):\n",
        "    # Open the NetCDF file\n",
        "    dataset = nc.Dataset(nc_file)\n",
        "\n",
        "    # Extract basic information\n",
        "    variables = list(dataset.variables.keys())\n",
        "    time_range = None\n",
        "    if 'time' in dataset.variables:\n",
        "        time_range = (dataset.variables['time'][:].min(), dataset.variables['time'][:].max())\n",
        "\n",
        "    latitudes = dataset.variables['lat'][:] if 'lat' in dataset.variables else None\n",
        "    longitudes = dataset.variables['lon'][:] if 'lon' in dataset.variables else None\n",
        "    depth_range = None\n",
        "    if 'depth' in dataset.variables:\n",
        "        depth_range = (dataset.variables['depth'][:].min(), dataset.variables['depth'][:].max())\n",
        "\n",
        "    # Spatial resolution (assuming lat and lon are 1D arrays)\n",
        "    lat_resolution = latitudes[1] - latitudes[0] if latitudes is not None and len(latitudes) > 1 else None\n",
        "    lon_resolution = longitudes[1] - longitudes[0] if longitudes is not None and len(longitudes) > 1 else None\n",
        "\n",
        "    # Check for missing values in each variable\n",
        "    missing_value_info = {}\n",
        "    for var in variables:\n",
        "        if hasattr(dataset.variables[var], '_FillValue'):\n",
        "            missing_value_info[var] = dataset.variables[var].__dict__.get('_FillValue', None)\n",
        "\n",
        "    # Create a metadata dictionary\n",
        "    metadata = {\n",
        "        \"variables\": variables,\n",
        "        \"time_range\": time_range,\n",
        "        \"lat_resolution\": lat_resolution,\n",
        "        \"lon_resolution\": lon_resolution,\n",
        "        \"depth_range\": depth_range,\n",
        "        \"missing_value_info\": missing_value_info\n",
        "    }\n",
        "\n",
        "    return metadata\n",
        "\n",
        "# Function to handle specific queries based on extracted metadata\n",
        "def handle_query(query, metadata):\n",
        "    if \"variables\" in query.lower():\n",
        "        return f\"The variables in the NetCDF file are: {', '.join(metadata['variables'])}\"\n",
        "\n",
        "    elif \"time range\" in query.lower():\n",
        "        if metadata['time_range']:\n",
        "            return f\"The time range of the data is from {metadata['time_range'][0]} to {metadata['time_range'][1]}.\"\n",
        "        else:\n",
        "            return \"No time data available.\"\n",
        "\n",
        "    elif \"spatial resolution\" in query.lower():\n",
        "        if metadata['lat_resolution'] and metadata['lon_resolution']:\n",
        "            return f\"The spatial resolution is approximately {metadata['lat_resolution']} degrees in latitude and {metadata['lon_resolution']} degrees in longitude.\"\n",
        "        else:\n",
        "            return \"No spatial resolution data available.\"\n",
        "\n",
        "    elif \"missing values\" in query.lower():\n",
        "        if metadata['missing_value_info']:\n",
        "            missing_info = ', '.join([f\"{var}: {val}\" for var, val in metadata['missing_value_info'].items()])\n",
        "            return f\"The following variables have missing values: {missing_info}.\"\n",
        "        else:\n",
        "            return \"There are no missing values in the variables.\"\n",
        "\n",
        "    elif \"depth range\" in query.lower():\n",
        "        if metadata['depth_range']:\n",
        "            return f\"The depth range in this dataset is from {metadata['depth_range'][0]} to {metadata['depth_range'][1]} meters.\"\n",
        "        else:\n",
        "            return \"No depth data available.\"\n",
        "\n",
        "    else:\n",
        "        return \"Query not recognized or supported.\"\n",
        "\n",
        "# Example of running multiple queries on the metadata\n",
        "def run_queries(nc_file, queries):\n",
        "    # Extract metadata from the NetCDF file\n",
        "    metadata = extract_metadata(nc_file)\n",
        "\n",
        "    # Handle each query and print the response\n",
        "    for query in queries:\n",
        "        print(f\"Question: {query}\")\n",
        "        response = handle_query(query, metadata)\n",
        "        print(f\"Response: {response}\\n\")\n",
        "\n",
        "# Define your queries\n",
        "queries = [\n",
        "    \"What are the variables in the NetCDF file?\",\n",
        "    \"What is the time range of the data in the NetCDF file?\",\n",
        "    \"What is the spatial resolution of the data in the file?\",\n",
        "    \"Are there any missing values in the NetCDF file?\",\n",
        "    \"What is the depth range covered in this dataset?\"\n",
        "]\n",
        "\n",
        "# Path to your NetCDF file\n",
        "nc_file = '/content/gom_t008.nc'\n",
        "\n",
        "# Run the queries\n",
        "run_queries(nc_file, queries)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GC6rla-_5kGh",
        "outputId": "dd556090-e415-4cf7-9e7d-0393346f5604"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What are the variables in the NetCDF file?\n",
            "Response: The variables in the NetCDF file are: time, tau, depth, lat, lon, water_u, water_v, water_temp, salinity, surf_el\n",
            "\n",
            "Question: What is the time range of the data in the NetCDF file?\n",
            "Response: The time range of the data is from 192884.00000000006 to 192884.00000000006.\n",
            "\n",
            "Question: What is the spatial resolution of the data in the file?\n",
            "Response: The spatial resolution is approximately 0.03999900817871094 degrees in latitude and 0.03997802734375 degrees in longitude.\n",
            "\n",
            "Question: Are there any missing values in the NetCDF file?\n",
            "Response: The following variables have missing values: water_u: -30000, water_v: -30000, water_temp: -30000, salinity: -30000, surf_el: -30000.\n",
            "\n",
            "Question: What is the depth range covered in this dataset?\n",
            "Response: The depth range in this dataset is from 0.0 to 5000.0 meters.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fwR436Je5niG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}